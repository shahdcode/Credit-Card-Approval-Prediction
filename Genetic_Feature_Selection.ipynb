{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahdcode/Credit-Card-Approval-Prediction/blob/main/Genetic_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "-q-w9tnVN-vf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qWoSbYQgDYmZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algrotihms\n"
      ],
      "metadata": {
        "id": "KsLYObk3OET3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particle Swarm Optimization (PSO)"
      ],
      "metadata": {
        "id": "EGCFmGBPU5Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def fitness_function(features, X, y, classifier):\n",
        "#     \"\"\"\n",
        "#     Evaluate the fitness of a particle.\n",
        "\n",
        "#     Args:\n",
        "#         features (array): Binary array representing selected features.\n",
        "#         X (ndarray): Feature matrix.\n",
        "#         y (ndarray): Target array.\n",
        "#         classifier: A machine learning model (e.g., DecisionTreeClassifier).\n",
        "\n",
        "#     Returns:\n",
        "#         float: The fitness value (lower is better for minimization tasks).\n",
        "#     \"\"\"\n",
        "#     # Select features based on the binary array\n",
        "#     selected_features = X[:, features == 1]\n",
        "\n",
        "#     # If no features are selected, return a high fitness value\n",
        "#     if selected_features.shape[1] == 0:\n",
        "#         return float('inf')\n",
        "\n",
        "#     # Perform a simple train-test split (80%-20%)\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#     # Train the classifier\n",
        "#     classifier.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict and calculate accuracy\n",
        "#     y_pred = classifier.predict(X_test)\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#     # Return the negative accuracy as fitness (minimization task)\n",
        "#     return -accuracy\n",
        "\n",
        "\n",
        "\n",
        "def fitness_function(features, X, y, classifier):\n",
        "    \"\"\"\n",
        "    Evaluate the fitness of a particle.\n",
        "\n",
        "    Args:\n",
        "        features (array): Binary array representing selected features.\n",
        "        X (ndarray): Feature matrix.\n",
        "        y (ndarray): Target array.\n",
        "        classifier: A machine learning model (e.g., DecisionTreeClassifier).\n",
        "\n",
        "    Returns:\n",
        "        float: The fitness value (lower is better for minimization tasks).\n",
        "    \"\"\"\n",
        "    # Select features based on the binary array\n",
        "    selected_features = X.iloc[:, features == 1]\n",
        "\n",
        "    # If no features are selected, return a high fitness value\n",
        "    if selected_features.shape[1] == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # Perform a simple train-test split (80%-20%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Return the negative accuracy as fitness (minimization task)\n",
        "    return -accuracy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pX9XWjo3W1yf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_swarm(n_particles, n_features, bounds, fitness_function, X, y, classifier):\n",
        "    \"\"\"\n",
        "    Initialize the swarm for PSO.\n",
        "\n",
        "    Args:\n",
        "        n_particles (int): Number of particles.\n",
        "        n_features (int): Number of features.\n",
        "        bounds (tuple): Bounds for the feature mask (binary: 0 or 1).\n",
        "        fitness_function (callable): The fitness function.\n",
        "        X (ndarray): Feature matrix.\n",
        "        y (ndarray): Target array.\n",
        "        classifier: A machine learning model.\n",
        "\n",
        "    Returns:\n",
        "        positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness\n",
        "    \"\"\"\n",
        "    lower_bounds, upper_bounds = bounds\n",
        "\n",
        "    # Randomly initialize positions (binary: 0 or 1) and velocities\n",
        "    positions = np.random.randint(lower_bounds, upper_bounds + 1, size=(n_particles, n_features))\n",
        "    velocities = np.random.uniform(-1, 1, (n_particles, n_features))\n",
        "\n",
        "    # Evaluate initial fitness\n",
        "    fitness = np.array([fitness_function(positions[i], X, y, classifier) for i in range(n_particles)])\n",
        "\n",
        "    # Set personal bests (pbest)\n",
        "    pbest_positions = positions.copy()\n",
        "    pbest_fitness = fitness.copy()\n",
        "\n",
        "    # Set global best (gbest)\n",
        "    gbest_index = np.argmin(pbest_fitness)\n",
        "    gbest_position = pbest_positions[gbest_index]\n",
        "    gbest_fitness = pbest_fitness[gbest_index]\n",
        "\n",
        "    return positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness\n"
      ],
      "metadata": {
        "id": "5FSJe6s-W-Aw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_best_solution(gbest_position, gbest_fitness, feature_names):\n",
        "#     \"\"\"\n",
        "#     Return the best solution found by the swarm.\n",
        "\n",
        "#     Args:\n",
        "#         gbest_position (array): The best feature mask.\n",
        "#         gbest_fitness (float): The best fitness value.\n",
        "#         feature_names (list): List of feature names.\n",
        "\n",
        "#     Returns:\n",
        "#         dict: A dictionary containing selected features and their fitness.\n",
        "#     \"\"\"\n",
        "#     selected_features = [feature_names[i] for i in range(len(gbest_position)) if gbest_position[i] == 1]\n",
        "#     return {\n",
        "#         \"Selected Features\": selected_features,\n",
        "#         \"Best Fitness\": -gbest_fitness  # Accuracy is the positive value of fitness\n",
        "#     }\n",
        "\n",
        "def return_best_solution(gbest_position, gbest_fitness, feature_names):\n",
        "    \"\"\"\n",
        "    Return the best solution found by the swarm.\n",
        "\n",
        "    Args:\n",
        "        gbest_position (array): The best feature mask.\n",
        "        gbest_fitness (float): The best fitness value.\n",
        "        feature_names (list): List of feature names.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing selected features and their fitness.\n",
        "    \"\"\"\n",
        "    selected_features = [feature_names[i] for i in range(len(feature_names)) if gbest_position[i] == 1]\n",
        "    return {\n",
        "        \"Selected Features\": selected_features,\n",
        "        \"Best Fitness\": -gbest_fitness  # Accuracy is the positive value of fitness\n",
        "    }\n"
      ],
      "metadata": {
        "id": "kO45L-wnW_mr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shahd\n",
        "def update_swarm(positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position, w, c1, c2, bounds, X, y, classifier):\n",
        "    lower_bounds, upper_bounds = bounds\n",
        "    n_particles, dimensions = positions.shape\n",
        "\n",
        "    # Temporarily cast positions to float64 for update step\n",
        "    positions = positions.astype(np.float64)\n",
        "\n",
        "    for i in range(n_particles):\n",
        "        # Update velocity\n",
        "        inertia = w * velocities[i]\n",
        "        cognitive = c1 * np.random.random() * (pbest_positions[i] - positions[i])\n",
        "        social = c2 * np.random.random() * (gbest_position - positions[i])\n",
        "        velocities[i] = inertia + cognitive + social\n",
        "\n",
        "        # Update position\n",
        "        positions[i] += velocities[i]\n",
        "\n",
        "        # Ensure positions stay within bounds (binary: 0 or 1)\n",
        "        positions[i] = np.clip(positions[i], lower_bounds, upper_bounds)\n",
        "\n",
        "    # After update, cast positions back to int64 for binary (0, 1)\n",
        "    positions = np.round(positions).astype(np.int64)\n",
        "\n",
        "    # Evaluate fitness\n",
        "    fitness = np.array([fitness_function(positions[i], X, y, classifier) for i in range(n_particles)])\n",
        "\n",
        "    # Update pbest\n",
        "    for i in range(n_particles):\n",
        "        if fitness[i] < pbest_fitness[i]:\n",
        "            pbest_fitness[i] = fitness[i]\n",
        "            pbest_positions[i] = positions[i]\n",
        "\n",
        "    # Update gbest\n",
        "    gbest_index = np.argmin(fitness)\n",
        "    if fitness[gbest_index] < np.min(pbest_fitness):\n",
        "        gbest_position = positions[gbest_index]\n",
        "\n",
        "    return positions, velocities, pbest_positions, pbest_fitness, gbest_position\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AOLz7TzpI5vg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genetic Algorithm Implementation"
      ],
      "metadata": {
        "id": "Giuxc_6sZNtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1 - Shahd\n",
        "def encode_solution_space(num_features):\n",
        "    \"\"\"Creates binary encoding for feature selection.\"\"\"\n",
        "    return np.random.choice([0, 1], size=num_features)\n"
      ],
      "metadata": {
        "id": "iGAZQYP0Go55"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2 - Habiba\n",
        "def set_algorithm_parameters():\n",
        "    \"\"\"Set the parameters for the Genetic Algorithm.\"\"\"\n",
        "    return {\n",
        "        \"pop_size\": 100,\n",
        "        \"num_generations\": 150,\n",
        "        \"crossover_rate\": 0.9,\n",
        "        \"mutation_rate\": 0.02\n",
        "    }"
      ],
      "metadata": {
        "id": "_EglotfOZVFJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3 - Shahd\n",
        "def create_initial_population(pop_size, num_features):\n",
        "    \"\"\"Generates the initial population of chromosomes.\"\"\"\n",
        "    return [encode_solution_space(num_features) for _ in range(pop_size)]"
      ],
      "metadata": {
        "id": "KNJ9TMGfGwob"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4 - Shahd\n",
        "\n",
        "def measure_fitness(population, X_train, y_train, X_val, y_val, dynamic_k):\n",
        "    \"\"\"Evaluates the fitness of each chromosome with a dynamic k value.\"\"\"\n",
        "    fitness_scores = []\n",
        "    for chromosome in population:\n",
        "        selected_features = [i for i, bit in enumerate(chromosome) if bit == 1]\n",
        "        if not selected_features:\n",
        "            fitness_scores.append(0)\n",
        "            continue\n",
        "        X_train_selected = X_train.iloc[:, selected_features]\n",
        "        X_val_selected = X_val.iloc[:, selected_features]\n",
        "\n",
        "        # Train the model with the dynamic k value\n",
        "        model = KNeighborsClassifier(n_neighbors=dynamic_k)\n",
        "        model.fit(X_train_selected, y_train)\n",
        "        predictions = model.predict(X_val_selected)\n",
        "        fitness_scores.append(accuracy_score(y_val, predictions))\n",
        "    return fitness_scores\n",
        "\n",
        "\n",
        "# def measure_fitness(population, X_train, y_train, X_val, y_val, best_k=5):\n",
        "#     \"\"\"Evaluates the fitness of each chromosome (feature selection) in the population.\"\"\"\n",
        "#     fitness_scores = []\n",
        "#     for chromosome in population:\n",
        "#         selected_features = [i for i, bit in enumerate(chromosome) if bit == 1]\n",
        "#         if not selected_features:\n",
        "#             fitness_scores.append(0)\n",
        "#             continue\n",
        "#         X_train_selected = X_train.iloc[:, selected_features]\n",
        "#         X_val_selected = X_val.iloc[:, selected_features]\n",
        "\n",
        "#         # Train the model with the default value for k (e.g., k=5)\n",
        "#         model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "#         model.fit(X_train_selected, y_train)\n",
        "#         predictions = model.predict(X_val_selected)\n",
        "#         fitness_scores.append(accuracy_score(y_val, predictions))\n",
        "#     return fitness_scores\n"
      ],
      "metadata": {
        "id": "FAWSwijVG1m8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5 - Habiba\n",
        "# def select_parents(population, fitness_values):\n",
        "#     \"\"\"Select parents using Tournament Selection.\"\"\"\n",
        "#     parents = []\n",
        "#     for _ in range(len(population)):\n",
        "#         # Tournament selection: Randomly pick 3 and select the best\n",
        "#         candidates_idx = np.random.choice(len(population), 3, replace=False)\n",
        "#         best_candidate = max(candidates_idx, key=lambda idx: fitness_values[idx])\n",
        "#         parents.append(population[best_candidate])\n",
        "#     return np.array(parents)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def select_parents(population, fitness_values):\n",
        "    \"\"\"Select parents using Roulette Wheel Selection.\"\"\"\n",
        "    total_fitness = sum(fitness_values)\n",
        "    selection_probs = [f / total_fitness for f in fitness_values]\n",
        "    parents = np.random.choice(population, size=len(population), p=selection_probs)\n",
        "    return np.array(parents)\n"
      ],
      "metadata": {
        "id": "a5zT8I3ab95R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6 - Habiba\n",
        "def crossover(parents, crossover_rate):\n",
        "    \"\"\"Apply single-point crossover.\"\"\"\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        parent1, parent2 = parents[i], parents[(i + 1) % len(parents)]\n",
        "        if np.random.rand() < crossover_rate:\n",
        "            point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate([parent1[:point], parent2[point:]])\n",
        "            child2 = np.concatenate([parent2[:point], parent1[point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "        else:\n",
        "            offspring.extend([parent1, parent2])\n",
        "    return np.array(offspring)\n"
      ],
      "metadata": {
        "id": "vPYPh9wob-eq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7 - Habiba\n",
        "def populate_next_generation(offspring, fitness_values):\n",
        "    \"\"\"Replace population with offspring and apply elitism.\"\"\"\n",
        "    elite_idx = np.argmax(fitness_values)\n",
        "    next_generation = offspring\n",
        "    next_generation[0] = offspring[elite_idx]  # Ensure elite survives\n",
        "    return next_generation"
      ],
      "metadata": {
        "id": "psBjdg6fcF4h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8 - Habiba\n",
        "def mutate(offspring, mutation_rate):\n",
        "    \"\"\"Apply bit-flip mutation.\"\"\"\n",
        "    for i in range(len(offspring)):\n",
        "        for j in range(len(offspring[i])):\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                offspring[i][j] = 1 - offspring[i][j]  # Flip bit\n",
        "    return offspring\n"
      ],
      "metadata": {
        "id": "hq0RNObNcFQa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9 - Shahd\n",
        "# def check_stopping_condition(generation, max_generations, fitness_scores, threshold, no_change_limit, previous_scores):\n",
        "#     \"\"\"Checks if the algorithm should stop based on fitness, generations, or stagnation.\"\"\"\n",
        "#     if generation >= max_generations or max(fitness_scores) >= threshold:\n",
        "#         return True\n",
        "#     if len(previous_scores) >= no_change_limit and all(score == previous_scores[0] for score in previous_scores):\n",
        "#         return True\n",
        "#     return False\n",
        "\n",
        "\n",
        "def check_stopping_condition(generation, max_generations, fitness_scores, threshold, no_change_limit, previous_scores):\n",
        "    \"\"\"Checks if the algorithm should stop based on fitness, generations, or stagnation.\"\"\"\n",
        "    if generation >= max_generations or max(fitness_scores) >= threshold:\n",
        "        return True\n",
        "    if len(previous_scores) >= no_change_limit and all(score == previous_scores[0] for score in previous_scores):\n",
        "        return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "KxcB3rUmG5PH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm(X_train, y_train, X_val, y_val, num_features):\n",
        "    \"\"\"Runs the genetic algorithm for feature selection with a dynamic k.\"\"\"\n",
        "    params = set_algorithm_parameters()\n",
        "    params['no_change_limit'] = 10  # Define stagnation limit\n",
        "    population = create_initial_population(params['pop_size'], num_features)\n",
        "    previous_scores = []\n",
        "\n",
        "    for generation in range(params['num_generations']):\n",
        "        # Dynamically compute the best k for the current generation\n",
        "        dynamic_k, _ = find_optimal_k(X_train, y_train)\n",
        "\n",
        "        # Evaluate fitness with the dynamic k value\n",
        "        fitness_scores = measure_fitness(population, X_train, y_train, X_val, y_val, dynamic_k)\n",
        "\n",
        "        if check_stopping_condition(generation, params['num_generations'], fitness_scores, 0.95, params['no_change_limit'], previous_scores):\n",
        "            break\n",
        "\n",
        "        previous_scores.append(max(fitness_scores))\n",
        "        if len(previous_scores) > params['no_change_limit']:\n",
        "            previous_scores.pop(0)\n",
        "\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents, params['crossover_rate'])\n",
        "        offspring = mutate(offspring, params['mutation_rate'])\n",
        "        population = populate_next_generation(offspring, fitness_scores)\n",
        "\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return best_solution, max(fitness_scores)  # Return best solution and fitness score\n",
        "\n"
      ],
      "metadata": {
        "id": "FGKav0l0Huy1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knn"
      ],
      "metadata": {
        "id": "GrVJKEH8NzO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_k(X_train, y_train):\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    param_grid = {'n_neighbors': list(range(1, 31))}\n",
        "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_params_['n_neighbors'], grid_search.best_score_\n"
      ],
      "metadata": {
        "id": "XGFx9norODWB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knn_and_display_accuracy(X_train, y_train, X_val, y_val, X_test, y_test, best_k):\n",
        "    # Initialize the KNN model with the optimal k\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "\n",
        "    # Train the model on the training set\n",
        "    knn_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_val_pred = knn_model.predict(X_val)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_test_pred = knn_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy on the validation set\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "    # Calculate accuracy on the test set\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    # Display the accuracies\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "IrBZQRhjhTl4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def separate_features_targets(train_df, val_df, test_df):\n",
        "    \"\"\"\n",
        "    Function to separate features and target from three datasets (train, validation, and test).\n",
        "\n",
        "    Parameters:\n",
        "    - train_df: DataFrame for training data\n",
        "    - val_df: DataFrame for validation data\n",
        "    - test_df: DataFrame for test data\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_val, X_test: Features for train, validation, and test sets\n",
        "    - y_train, y_val, y_test: Target labels for train, validation, and test sets\n",
        "    \"\"\"\n",
        "    # Separate features and target for training set\n",
        "    X_train = train_df.drop(columns=['status_mapped'])\n",
        "    y_train = train_df['status_mapped']\n",
        "\n",
        "    # Separate features and target for validation set\n",
        "    X_val = val_df.drop(columns=['status_mapped'])\n",
        "    y_val = val_df['status_mapped']\n",
        "\n",
        "    # Separate features and target for test set\n",
        "    X_test = test_df.drop(columns=['status_mapped'])\n",
        "    y_test = test_df['status_mapped']\n",
        "\n",
        "    # Return all the variables\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "MOqZKbNa64GT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_scaled_df=pd.read_csv('/content/drive/MyDrive/Ai project/train_scaled.csv')\n",
        "val_scaled=pd.read_csv('/content/drive/MyDrive/Ai project/validation_scaled.csv')\n",
        "test_scaled=pd.read_csv('/content/drive/MyDrive/Ai project/test_scaled.csv')\n",
        "\n",
        "# Load the data\n",
        "\n",
        "# Drop the 'ID' column\n",
        "train_scaled_df = train_scaled_df.drop(columns=['ID'])\n",
        "val_scaled = val_scaled.drop(columns=['ID'])\n",
        "test_scaled = test_scaled.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "dm4vJKAlXygU"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the column names of the scaled datasets\n",
        "print(\"Train DataFrame Columns:\", train_scaled_df.columns)\n",
        "print(\"Validation DataFrame Columns:\", val_scaled.columns)\n",
        "print(\"Test DataFrame Columns:\", test_scaled.columns)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPZYFrkBbkau",
        "outputId": "6dc9d204-8c05-4962-8072-129054873841"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataFrame Columns: Index(['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN',\n",
            "       'AMT_INCOME_TOTAL', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
            "       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL',\n",
            "       'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE',\n",
            "       'CNT_FAM_MEMBERS', 'AGE_YEARS', 'YEARS_EMPLOYED', 'status_mapped'],\n",
            "      dtype='object')\n",
            "Validation DataFrame Columns: Index(['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN',\n",
            "       'AMT_INCOME_TOTAL', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
            "       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL',\n",
            "       'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE',\n",
            "       'CNT_FAM_MEMBERS', 'AGE_YEARS', 'YEARS_EMPLOYED', 'status_mapped'],\n",
            "      dtype='object')\n",
            "Test DataFrame Columns: Index(['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN',\n",
            "       'AMT_INCOME_TOTAL', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n",
            "       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL',\n",
            "       'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE',\n",
            "       'CNT_FAM_MEMBERS', 'AGE_YEARS', 'YEARS_EMPLOYED', 'status_mapped'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the split_data function\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df, val_scaled, test_scaled)\n",
        "\n",
        "# Run PSO for feature selection (same as previous code)\n",
        "classifier = KNeighborsClassifier()\n",
        "n_particles = 30\n",
        "n_features = X_train.shape[1]\n",
        "bounds = (0, 1)  # Binary feature selection\n",
        "\n",
        "positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness = initialize_swarm(\n",
        "    n_particles, n_features, bounds, fitness_function, X_train, y_train, classifier\n",
        ")\n",
        "\n",
        "# Track the PSO process using tqdm\n",
        "for _ in tqdm(range(100), desc=\"PSO Iterations\"):  # Max iterations\n",
        "    positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n",
        "        positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position,\n",
        "        w=0.5, c1=1.5, c2=1.5, bounds=bounds, X=X_train, y=y_train, classifier=classifier\n",
        "    )\n",
        "\n",
        "pso_result = return_best_solution(gbest_position, gbest_fitness, X_train.columns)\n",
        "selected_features_pso = pso_result[\"Selected Features\"]\n",
        "print(\"PSO Best Features:\", selected_features_pso)\n",
        "print(\"PSO Best Fitness (Accuracy):\", pso_result[\"Best Fitness\"])\n",
        "\n",
        "# Filter training, validation, and test sets by PSO-selected features\n",
        "X_train_selected_pso = X_train[selected_features_pso]\n",
        "X_val_selected_pso = X_val[selected_features_pso]\n",
        "X_test_selected_pso = X_test[selected_features_pso]\n",
        "\n",
        "# Train and evaluate KNN with PSO-selected features\n",
        "best_k, best_score = find_optimal_k(X_train_selected_pso, y_train)\n",
        "print(f\"Optimal k (PSO): {best_k} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "knn_model_pso = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_model_pso.fit(X_train_selected_pso, y_train)\n",
        "y_pred_pso = knn_model_pso.predict(X_val_selected_pso)\n",
        "accuracy_pso = accuracy_score(y_val, y_pred_pso)\n",
        "print(f\"Accuracy of KNN with PSO-selected features and k={best_k}: {accuracy_pso}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "Kn5fvsukH2Rj",
        "outputId": "108c359c-cadd-482c-8554-fb04fa50bfa8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_scaled_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-910c3a8cab5f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the split_data function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_features_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run PSO for feature selection (same as previous code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_scaled_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and targets\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df, val_scaled, test_scaled)\n",
        "\n",
        "# Run Genetic Algorithm for feature selection (without passing best_k)\n",
        "ga_best_solution, ga_best_fitness = genetic_algorithm(X_train, y_train, X_val, y_val, num_features=X_train.shape[1])\n",
        "\n",
        "# Print the best solution from the genetic algorithm\n",
        "print(\"GA Best Solution:\", ga_best_solution)\n",
        "\n",
        "# Get the names of the selected features (those with a 1 in ga_best_solution)\n",
        "ga_selected_columns = [X_train.columns[i] for i, bit in enumerate(ga_best_solution) if bit == 1]\n",
        "\n",
        "# Print the selected columns\n",
        "print(\"GA Selected Columns:\", ga_selected_columns)\n",
        "\n",
        "# Filter the training, validation, and test sets by GA-selected features using column names\n",
        "X_train_selected_ga = X_train[ga_selected_columns]\n",
        "X_val_selected_ga = X_val[ga_selected_columns]\n",
        "X_test_selected_ga = X_test[ga_selected_columns]\n",
        "\n",
        "# Convert to numpy arrays if needed\n",
        "X_train_selected_ga = X_train_selected_ga.values\n",
        "X_val_selected_ga = X_val_selected_ga.values\n",
        "X_test_selected_ga = X_test_selected_ga.values\n",
        "\n",
        "# Print the GA selected features\n",
        "print(\"GA Selected Features:\", ga_selected_columns)\n",
        "print(\"GA Best Fitness (Accuracy):\", ga_best_fitness)\n",
        "\n",
        "# Find optimal k for KNN after feature selection\n",
        "best_k, best_score = find_optimal_k(X_train_selected_ga, y_train)\n",
        "print(f\"Optimal k (GA): {best_k} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "# Train the KNN model and display the accuracy\n",
        "train_knn_and_display_accuracy(X_train_selected_ga, y_train, X_val_selected_ga, y_val, X_test_selected_ga, y_test, best_k)"
      ],
      "metadata": {
        "id": "jxxGykCM7Ij_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ACr8k6skjO6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "W14pZj7suCWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Function to find the optimal number of estimators for Random Forest\n",
        "def find_optimal_estimators(X_train, y_train):\n",
        "    param_grid = {'n_estimators': list(range(10, 201, 10))}\n",
        "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_params_['n_estimators'], grid_search.best_score_\n",
        "\n",
        "# Function to split the data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df,val_scaled,test_scaled)\n",
        "\n",
        "# PSO for feature selection (placeholder functions for initialization and update)\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "n_particles = 30\n",
        "n_features = X_train.shape[1]\n",
        "bounds = (0, 1)  # Binary feature selection\n",
        "\n",
        "positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness = initialize_swarm(\n",
        "    n_particles, n_features, bounds, fitness_function, X_train, y_train, classifier\n",
        ")\n",
        "\n",
        "# Track the PSO process using tqdm\n",
        "for _ in tqdm(range(100), desc=\"PSO Iterations\"):\n",
        "    positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n",
        "        positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position,\n",
        "        w=0.5, c1=1.5, c2=1.5, bounds=bounds, X=X_train, y=y_train, classifier=classifier\n",
        "    )\n",
        "\n",
        "pso_result = return_best_solution(gbest_position, gbest_fitness, df.drop(columns=['status_mapped']).columns)\n",
        "selected_features_pso = pso_result[\"Selected Features\"]\n",
        "print(\"PSO Best Features:\", selected_features_pso)\n",
        "print(\"PSO Best Fitness (Accuracy):\", pso_result[\"Best Fitness\"])\n",
        "\n",
        "# Filter training, validation, and test sets by PSO-selected features\n",
        "X_train_selected_pso = X_train[:, gbest_position.astype(bool)]\n",
        "X_val_selected_pso = X_val[:, gbest_position.astype(bool)]\n",
        "X_test_selected_pso = X_test[:, gbest_position.astype(bool)]\n",
        "\n",
        "# Train and evaluate Random Forest with PSO-selected features\n",
        "best_estimators, best_score = find_optimal_estimators(X_train_selected_pso, y_train)\n",
        "print(f\"Optimal number of estimators (PSO): {best_estimators} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "rf_model_pso = RandomForestClassifier(n_estimators=best_estimators, random_state=42)\n",
        "rf_model_pso.fit(X_train_selected_pso, y_train)\n",
        "y_pred_pso = rf_model_pso.predict(X_val_selected_pso)\n",
        "accuracy_pso = accuracy_score(y_val, y_pred_pso)\n",
        "print(f\"Accuracy of Random Forest with PSO-selected features: {accuracy_pso}\")\n",
        "\n",
        "# Genetic Algorithm (GA) for feature selection\n",
        "# Placeholder genetic_algorithm function\n",
        "# ga_best_solution, ga_best_fitness = genetic_algorithm(X_train, y_train, X_val, y_val, num_features=X_train.shape[1])\n",
        "# ga_selected_features = [df.drop(columns=['status_mapped']).columns[i] for i, bit in enumerate(ga_best_solution) if bit == 1]\n",
        "# print(\"GA Best Features:\", ga_selected_features)\n",
        "# print(\"GA Best Fitness (Accuracy):\", ga_best_fitness)\n",
        "\n",
        "# # Filter training, validation, and test sets by GA-selected features\n",
        "# X_train_selected_ga = X_train[:, ga_best_solution.astype(bool)]\n",
        "# X_val_selected_ga = X_val[:, ga_best_solution.astype(bool)]\n",
        "# X_test_selected_ga = X_test[:, ga_best_solution.astype(bool)]\n",
        "\n",
        "# # Train and evaluate Random Forest with GA-selected features\n",
        "# best_estimators, best_score = find_optimal_estimators(X_train_selected_ga, y_train)\n",
        "# print(f\"Optimal number of estimators (GA): {best_estimators} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "# rf_model_ga = RandomForestClassifier(n_estimators=best_estimators, random_state=42)\n",
        "# rf_model_ga.fit(X_train_selected_ga, y_train)\n",
        "# y_pred_ga = rf_model_ga.predict(X_val_selected_ga)\n",
        "# accuracy_ga = accuracy_score(y_val, y_pred_ga)\n",
        "# print(f\"Accuracy of Random Forest with GA-selected features: {accuracy_ga}\")\n",
        "\n",
        "# # Save the datasets as CSV\n",
        "# train_df = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name='status_mapped')], axis=1)\n",
        "# val_df = pd.concat([pd.DataFrame(X_val), pd.Series(y_val, name='status_mapped')], axis=1)\n",
        "# test_df = pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name='status_mapped')], axis=1)\n",
        "\n",
        "# train_df.to_csv('/content/drive/MyDrive/Ai project/train_rf.csv', index=False)\n",
        "# val_df.to_csv('/content/drive/MyDrive/Ai project/validation_rf.csv', index=False)\n",
        "# test_df.to_csv('/content/drive/MyDrive/Ai project/test_rf.csv', index=False)\n",
        "\n",
        "# print(\"Datasets saved successfully:\")\n",
        "# print(f\"Train set: {train_df.shape}\")\n",
        "# print(f\"Validation set: {val_df.shape}\")\n",
        "# print(f\"Test set: {test_df.shape}\")\n"
      ],
      "metadata": {
        "id": "xzx7WBKxuCgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to find the optimal C and kernel for SVM\n",
        "def find_optimal_svm(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_params_, grid_search.best_score_\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Ai project/Feature_eng_updated.csv')\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=['status_mapped']).values  # Features as numpy array\n",
        "y = df['status_mapped'].values                # Target (label)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df,val_scaled,test_scaled)\n",
        "\n",
        "# Run PSO for feature selection\n",
        "classifier = SVC()\n",
        "n_particles = 30\n",
        "n_features = X_train.shape[1]\n",
        "bounds = (0, 1)  # Binary feature selection\n",
        "\n",
        "positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness = initialize_swarm(\n",
        "    n_particles, n_features, bounds, fitness_function, X_train, y_train, classifier\n",
        ")\n",
        "\n",
        "# Track the PSO process using tqdm\n",
        "for _ in tqdm(range(100), desc=\"PSO Iterations\"):  # Max iterations\n",
        "    positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n",
        "        positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position,\n",
        "        w=0.5, c1=1.5, c2=1.5, bounds=bounds, X=X_train, y=y_train, classifier=classifier\n",
        "    )\n",
        "\n",
        "pso_result = return_best_solution(gbest_position, gbest_fitness, df.drop(columns=['status_mapped']).columns)\n",
        "selected_features_pso = pso_result[\"Selected Features\"]\n",
        "print(\"PSO Best Features:\", selected_features_pso)\n",
        "print(\"PSO Best Fitness (Accuracy):\", pso_result[\"Best Fitness\"])\n",
        "\n",
        "# Filter training, validation, and test sets by PSO-selected features\n",
        "X_train_selected_pso = X_train[:, gbest_position.astype(bool)]\n",
        "X_val_selected_pso = X_val[:, gbest_position.astype(bool)]\n",
        "X_test_selected_pso = X_test[:, gbest_position.astype(bool)]\n",
        "\n",
        "# Find optimal parameters for SVM after PSO feature selection\n",
        "best_params, best_score = find_optimal_svm(X_train_selected_pso, y_train)\n",
        "print(f\"Optimal parameters (PSO): {best_params} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "# Train and evaluate SVM with PSO-selected features\n",
        "svm_model_pso = SVC(**best_params)\n",
        "svm_model_pso.fit(X_train_selected_pso, y_train)\n",
        "y_pred_pso = svm_model_pso.predict(X_val_selected_pso)\n",
        "accuracy_pso = accuracy_score(y_val, y_pred_pso)\n",
        "print(f\"Accuracy of SVM with PSO-selected features: {accuracy_pso}\")\n",
        "\n",
        "# # Run Genetic Algorithm for feature selection (without passing best_params)\n",
        "# ga_best_solution, ga_best_fitness = genetic_algorithm(X_train, y_train, X_val, y_val, num_features=X_train.shape[1])\n",
        "# ga_selected_features = [df.drop(columns=['status_mapped']).columns[i] for i, bit in enumerate(ga_best_solution) if bit == 1]\n",
        "# print(\"GA Best Features:\", ga_selected_features)\n",
        "# print(\"GA Best Fitness (Accuracy):\", ga_best_fitness)\n",
        "\n",
        "# # Filter training, validation, and test sets by GA-selected features\n",
        "# X_train_selected_ga = X_train[:, ga_best_solution.astype(bool)]\n",
        "# X_val_selected_ga = X_val[:, ga_best_solution.astype(bool)]\n",
        "# X_test_selected_ga = X_test[:, ga_best_solution.astype(bool)]\n",
        "\n",
        "# # Find optimal parameters for SVM after GA feature selection\n",
        "# best_params, best_score = find_optimal_svm(X_train_selected_ga, y_train)\n",
        "# print(f\"Optimal parameters (GA): {best_params} with cross-validation accuracy: {best_score}\")\n",
        "\n",
        "# # Train and evaluate SVM with GA-selected features\n",
        "# svm_model_ga = SVC(**best_params)\n",
        "# svm_model_ga.fit(X_train_selected_ga, y_train)\n",
        "# y_pred_ga = svm_model_ga.predict(X_val_selected_ga)\n",
        "# accuracy_ga = accuracy_score(y_val, y_pred_ga)\n",
        "# print(f\"Accuracy of SVM with GA-selected features: {accuracy_ga}\")\n",
        "\n",
        "# # Save the splits as CSV files (if needed, combine X and y for each split)\n",
        "# train_df = pd.concat([X_train, y_train], axis=1)\n",
        "# val_df = pd.concat([X_val, y_val], axis=1)\n",
        "# test_df = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# train_df.to_csv('/content/drive/MyDrive/Ai project/train_svm.csv', index=False)\n",
        "# val_df.to_csv('/content/drive/MyDrive/Ai project/validation_svm.csv', index=False)\n",
        "# test_df.to_csv('/content/drive/MyDrive/Ai project/test_svm.csv', index=False)\n",
        "\n",
        "# print(\"Datasets saved successfully:\")\n",
        "# print(f\"Train set: {train_df.shape}\")\n",
        "# print(f\"Validation set: {val_df.shape}\")\n",
        "# print(f\"Test set: {test_df.shape}\")\n"
      ],
      "metadata": {
        "id": "RxRt8zgp6NCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees"
      ],
      "metadata": {
        "id": "5SMBP9X0RFYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IcfdUqxFYWfN",
        "outputId": "6b88b922-6943-44e8-85e7-6830497f1b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_scaled_df=pd.read_csv('/content/drive/MyDrive/Ai project/train_scaled.csv')\n",
        "val_scaled=pd.read_csv('/content/drive/MyDrive/Ai project/validation_scaled.csv')\n",
        "test_scaled=pd.read_csv('/content/drive/MyDrive/Ai project/test_scaled.csv')\n",
        "\n",
        "# Load the data\n",
        "\n",
        "# Drop the 'ID' column\n",
        "train_scaled_df = train_scaled_df.drop(columns=['ID'])\n",
        "val_scaled = val_scaled.drop(columns=['ID'])\n",
        "test_scaled = test_scaled.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "WW_Yu5QguyEz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision trees model implementing genetic feature selection and Particle Swarm Optimization as well as Grid Search:\n",
        "\n",
        "def train_decision_tree(X_train, y_train, X_val, y_val, X_test, y_test, param_grid):\n",
        "    dt_model = DecisionTreeClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    train_score = accuracy_score(y_train, best_model.predict(X_train))\n",
        "    val_score = accuracy_score(y_val, best_model.predict(X_val))\n",
        "    test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "    print(f\"Best Parameters (Decision Tree): {best_params}\")\n",
        "    print(f\"Training Accuracy: {train_score}\")\n",
        "    print(f\"Validation Accuracy: {val_score}\")\n",
        "    print(f\"Testing Accuracy: {test_score}\")"
      ],
      "metadata": {
        "id": "jlUxB7-lfbSk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using Genetic Algorithm\n",
        "# Step 1: Separate features and targets\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df, val_scaled, test_scaled)\n",
        "\n",
        "# Step 2: Run Genetic Algorithm for feature selection\n",
        "ga_best_solution, ga_best_fitness = genetic_algorithm(X_train, y_train, X_val, y_val, num_features=X_train.shape[1])\n",
        "\n",
        "# Step 3: Print the best solution from the genetic algorithm\n",
        "print(\"GA Best Solution:\", ga_best_solution)\n",
        "\n",
        "# Step 4: Get the names of the selected features\n",
        "ga_selected_columns = [X_train.columns[i] for i, bit in enumerate(ga_best_solution) if bit == 1]\n",
        "\n",
        "# Step 5: Print the selected columns\n",
        "print(\"GA Selected Columns:\", ga_selected_columns)\n",
        "\n",
        "# Step 6: Filter the training, validation, and test sets by GA-selected features\n",
        "X_train_selected_ga = X_train[ga_selected_columns]\n",
        "X_val_selected_ga = X_val[ga_selected_columns]\n",
        "X_test_selected_ga = X_test[ga_selected_columns]\n",
        "\n",
        "# Step 7: Convert to numpy arrays if needed\n",
        "X_train_selected_ga = X_train_selected_ga.values\n",
        "X_val_selected_ga = X_val_selected_ga.values\n",
        "X_test_selected_ga = X_test_selected_ga.values\n",
        "\n",
        "# Step 8: Print the GA selected features\n",
        "print(\"GA Selected Features:\", ga_selected_columns)\n",
        "print(\"GA Best Fitness (Accuracy):\", ga_best_fitness)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "LsfJw3BjRR8o",
        "outputId": "1beb186d-99aa-4f99-f23a-1f00be93601f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-3645937fd1a2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Step 2: Run Genetic Algorithm for feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mga_best_solution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mga_best_fitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenetic_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Step 3: Print the best solution from the genetic algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3df14dcf936f>\u001b[0m in \u001b[0;36mgenetic_algorithm\u001b[0;34m(X_train, y_train, X_val, y_val, num_features)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_generations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Dynamically compute the best k for the current generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdynamic_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Evaluate fitness with the dynamic k value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-2fee1948bcd4>\u001b[0m in \u001b[0;36mfind_optimal_k\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_neighbors'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_neighbors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    971\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         test_scores = _score(\n\u001b[0m\u001b[1;32m    889\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_params_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;34m\"\"\"Method that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mMean\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    866\u001b[0m         )\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_pairwise_distances_reductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             results = ArgKmin.compute(\n\u001b[0m\u001b[1;32m    869\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             return ArgKmin64.compute(\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\u001b[0m in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_original_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using Particle Swarm Optimization\n",
        "classifier_dt = DecisionTreeClassifier()\n",
        "positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness = initialize_swarm(\n",
        "    n_particles, n_features, bounds, fitness_function, X_train, y_train, classifier_dt\n",
        ")\n",
        "\n",
        "for _ in tqdm(range(100), desc=\"PSO Iterations for Decision Tree\"):\n",
        "    positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n",
        "        positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position,\n",
        "        w=0.5, c1=1.5, c2=1.5, bounds=bounds, X=X_train, y=y_train, classifier=classifier_dt\n",
        "    )\n",
        "\n",
        "pso_result = return_best_solution(gbest_position, gbest_fitness, X_train.columns)\n",
        "selected_features_pso = pso_result[\"Selected Features\"]\n",
        "X_train_selected_pso = X_train[selected_features_pso]\n",
        "X_val_selected_pso = X_val[selected_features_pso]\n",
        "X_test_selected_pso = X_test[selected_features_pso]"
      ],
      "metadata": {
        "id": "hFhHXgAlffIM",
        "outputId": "2079f829-b29b-4900-b201-2718eaceeff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations for Decision Tree: 100%|| 100/100 [01:27<00:00,  1.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for tuning Decision Tree\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 3, 5],\n",
        "    'max_features': [None, 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Train Decision Tree with GA-selected features\n",
        "print(\"Training Decision Tree with GA-selected features:\")\n",
        "train_decision_tree(X_train_selected_ga, y_train, X_val_selected_ga, y_val, X_test_selected_ga, y_test, param_grid)\n",
        "\n",
        "# Train Decision Tree with PSO-selected features\n",
        "print(\"\\nTraining Decision Tree with PSO-selected features:\")\n",
        "train_decision_tree(X_train_selected_pso, y_train, X_val_selected_pso, y_val, X_test_selected_pso, y_test, param_grid)"
      ],
      "metadata": {
        "id": "0FJjPVYGZL-I",
        "outputId": "06e8f669-d331-4dd1-c545-4506620d0bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Decision Tree with GA-selected features:\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters (Decision Tree): {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
            "Training Accuracy: 0.9499600390729065\n",
            "Validation Accuracy: 0.9498549523414836\n",
            "Testing Accuracy: 0.949658172778123\n",
            "\n",
            "Training Decision Tree with PSO-selected features:\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters (Decision Tree): {'criterion': 'gini', 'max_depth': 3, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Training Accuracy: 0.9498712370126987\n",
            "Validation Accuracy: 0.9498549523414836\n",
            "Testing Accuracy: 0.9498653407913819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "docVFZIBYnsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GTNKpeRaZJDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def train_mlp(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Normalize the data\n",
        "    X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
        "    X_val = tf.keras.utils.normalize(X_val, axis=1)\n",
        "    X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
        "\n",
        "    # Define the MLP model\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))  # Input layer\n",
        "    model.add(tf.keras.layers.Dense(64, activation='sigmoid'))  # Hidden layer\n",
        "    model.add(tf.keras.layers.Dense(len(np.unique(y_train)), activation='softmax'))  # Output layer (multi-class)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "\n",
        "    # Save the model\n",
        "    model.save('mlp.keras')\n",
        "\n",
        "    # Load the model\n",
        "    model = tf.keras.models.load_model('mlp.keras')\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Test Loss: {loss:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wb8-toP0YpWq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using Genetic Algorithms for MLP\n",
        "\n",
        "\n",
        "# Step 1: Separate features and targets\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df, val_scaled, test_scaled)\n",
        "\n",
        "# Step 2: Run Genetic Algorithm for feature selection\n",
        "ga_best_solution, ga_best_fitness = genetic_algorithm(X_train, y_train, X_val, y_val, num_features=X_train.shape[1])\n",
        "\n",
        "# Step 3: Print the best solution from the genetic algorithm\n",
        "print(\"GA Best Solution:\", ga_best_solution)\n",
        "\n",
        "# Step 4: Get the names of the selected features\n",
        "ga_selected_columns = [X_train.columns[i] for i, bit in enumerate(ga_best_solution) if bit == 1]\n",
        "\n",
        "# Step 5: Print the selected columns\n",
        "print(\"GA Selected Columns:\", ga_selected_columns)\n",
        "\n",
        "# Step 6: Filter the training, validation, and test sets by GA-selected features\n",
        "X_train_selected_ga = X_train[ga_selected_columns]\n",
        "X_val_selected_ga = X_val[ga_selected_columns]\n",
        "X_test_selected_ga = X_test[ga_selected_columns]\n",
        "\n",
        "# Step 7: Convert to numpy arrays if needed\n",
        "X_train_selected_ga = X_train_selected_ga.values\n",
        "X_val_selected_ga = X_val_selected_ga.values\n",
        "X_test_selected_ga = X_test_selected_ga.values\n",
        "\n",
        "# Step 8: Print the GA selected features\n",
        "print(\"GA Selected Features:\", ga_selected_columns)\n",
        "print(\"GA Best Fitness (Accuracy):\", ga_best_fitness)"
      ],
      "metadata": {
        "id": "Z7fRI9Wie9TC",
        "outputId": "d125a4e6-83ec-4c99-baa7-ddca48313604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GA Best Solution: [0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0]\n",
            "GA Selected Columns: ['CNT_CHILDREN', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'AGE_YEARS']\n",
            "GA Selected Features: ['CNT_CHILDREN', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'AGE_YEARS']\n",
            "GA Best Fitness (Accuracy): 0.9500621632822213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def initialize_swarm(n_particles, n_features, bounds, fitness_function, X, y, classifier):\n",
        "    # Initialize positions and velocities randomly\n",
        "    positions = np.random.uniform(bounds[0], bounds[1], (n_particles, n_features))\n",
        "    velocities = np.random.uniform(-1, 1, (n_particles, n_features))\n",
        "\n",
        "    # Evaluate initial fitness\n",
        "    fitness = np.array([fitness_function(positions[i], X, y, classifier) for i in range(n_particles)])\n",
        "\n",
        "    # Set personal bests (pbest)\n",
        "    pbest_positions = np.copy(positions)\n",
        "    pbest_fitness = fitness.copy()\n",
        "\n",
        "    # Set global best (gbest)\n",
        "    gbest_position = positions[np.argmax(fitness)]\n",
        "    gbest_fitness = np.max(fitness)\n",
        "\n",
        "    return positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness\n",
        "\n",
        "# Call the split_data function\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = separate_features_targets(train_scaled_df, val_scaled, test_scaled)\n",
        "\n",
        "# Define the classifier\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(30,), max_iter=300, random_state=42)  # Simplified network with more iterations\n",
        "\n",
        "# Initialize PSO parameters\n",
        "n_particles = 10  # Reduce the number of particles\n",
        "n_features = X_train.shape[1]\n",
        "bounds = (0, 1)  # Binary feature selection\n",
        "\n",
        "# def fitness_function(position, X, y, classifier):\n",
        "#     # Select features based on the position\n",
        "#     selected_features = [i for i, bit in enumerate(position) if bit > 0.5]\n",
        "#     if len(selected_features) == 0:\n",
        "#         return 0\n",
        "\n",
        "#     X_selected = X.iloc[:, selected_features]\n",
        "\n",
        "#     # Train the classifier\n",
        "#     classifier.fit(X_selected, y)\n",
        "\n",
        "#     # Perform cross-validation\n",
        "#     scores = cross_val_score(classifier, X_selected, y, cv=3, n_jobs=-1)  # Enable parallel processing and reduce CV folds\n",
        "\n",
        "#     # Return the mean accuracy\n",
        "#     return scores.mean()\n",
        "\n",
        "# Initialize swarm\n",
        "positions, velocities, pbest_positions, pbest_fitness, gbest_position, gbest_fitness = initialize_swarm(\n",
        "    n_particles, n_features, bounds, fitness_function, X_train, y_train, classifier\n",
        ")\n",
        "\n",
        "# Track the PSO process using tqdm\n",
        "for _ in tqdm(range(30), desc=\"PSO Iterations\"):  # Further reduce the number of iterations\n",
        "    positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n",
        "        positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position,\n",
        "        w=0.5, c1=1.5, c2=1.5, bounds=bounds, X=X_train, y=y_train, classifier=classifier\n",
        "    )\n",
        "\n",
        "# Retrieve the best solution\n",
        "pso_result = return_best_solution(gbest_position, gbest_fitness, X_train.columns)\n",
        "selected_features_pso = pso_result[\"Selected Features\"]\n",
        "print(\"PSO Best Features:\", selected_features_pso)\n",
        "print(\"PSO Best Fitness (Accuracy):\", pso_result[\"Best Fitness\"])\n",
        "\n",
        "# Filter training, validation, and test sets by PSO-selected features\n",
        "X_train_selected_pso = X_train[selected_features_pso]\n",
        "X_val_selected_pso = X_val[selected_features_pso]\n",
        "X_test_selected_pso = X_test[selected_features_pso]\n",
        "\n",
        "# Train and evaluate MLP with PSO-selected features\n",
        "mlp_model_pso = MLPClassifier(hidden_layer_sizes=(30,), max_iter=300, random_state=42)\n",
        "mlp_model_pso.fit(X_train_selected_pso, y_train)\n",
        "y"
      ],
      "metadata": {
        "id": "9CD0053jfMEZ",
        "outputId": "b61200ce-142e-4aab-bac8-c6e5a7ad98f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:   3%|         | 1/30 [04:16<2:04:03, 256.67s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-3a1bb115e298>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Track the PSO process using tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PSO Iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Further reduce the number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     positions, velocities, pbest_positions, pbest_fitness, gbest_position = update_swarm(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitness_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbest_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbest_fitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbest_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ebabbfb38e7c>\u001b[0m in \u001b[0;36mupdate_swarm\u001b[0;34m(positions, velocities, fitness_function, pbest_positions, pbest_fitness, gbest_position, w, c1, c2, bounds, X, y, classifier)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Evaluate fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfitness_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Update pbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ebabbfb38e7c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Evaluate fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfitness_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Update pbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-f045717a1743>\u001b[0m in \u001b[0;36mfitness_function\u001b[0;34m(position, X, y, classifier)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Perform cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Enable parallel processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Return the mean accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train MLP with GA-selected features\n",
        "print(\"Training MLP with GA-selected features:\")\n",
        "train_mlp(X_train_selected_ga, y_train, X_val_selected_ga, y_val, X_test_selected_ga, y_test)\n",
        "\n",
        "# # Train MLP with PSO-selected features\n",
        "# print(\"\\nTraining MLP with PSO-selected features:\")\n",
        "# train_mlp(X_train_selected_pso, y_train, X_val_selected_pso, y_val, X_test_selected_pso, y_test)\n"
      ],
      "metadata": {
        "id": "tNvNxdkQY8IH",
        "outputId": "c9ce1265-b7df-46b5-ef6a-b2a6f6d848be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MLP with GA-selected features:\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.2133 - val_accuracy: 0.9499 - val_loss: 0.1991\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.2017 - val_accuracy: 0.9499 - val_loss: 0.1991\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9509 - loss: 0.1970 - val_accuracy: 0.9499 - val_loss: 0.1986\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9511 - loss: 0.1965 - val_accuracy: 0.9499 - val_loss: 0.1989\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9505 - loss: 0.1976 - val_accuracy: 0.9499 - val_loss: 0.1986\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9495 - loss: 0.2010 - val_accuracy: 0.9499 - val_loss: 0.1985\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1978 - val_accuracy: 0.9499 - val_loss: 0.1985\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9480 - loss: 0.2051 - val_accuracy: 0.9499 - val_loss: 0.1985\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9497 - loss: 0.1993 - val_accuracy: 0.9499 - val_loss: 0.1988\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9481 - loss: 0.2049 - val_accuracy: 0.9499 - val_loss: 0.1985\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9506 - loss: 0.1969\n",
            "Test Accuracy: 0.95\n",
            "Test Loss: 0.20\n"
          ]
        }
      ]
    }
  ]
}